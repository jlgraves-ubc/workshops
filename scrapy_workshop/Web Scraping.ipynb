{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c08264e-6480-4746-8e83-1fe7a293c6dd",
   "metadata": {},
   "source": [
    "# Introduction to Web Scraping using Scrapy\n",
    "\n",
    "## Author\n",
    "\n",
    "* Jonathan Graves (<jonathan.graves@ubc.ca>)\n",
    "\n",
    "## Pre-requisties\n",
    "\n",
    "* A PC or tablet \n",
    "* Firefox\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003fee5c-69a0-4f42-800b-b18d322fe4ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "# What is Web Scraping?\n",
    "\n",
    "When we talk about web scraping, we are usually refer to the use of automated tools and software which collect information automatically from websites.  This occurs in three main ways:\n",
    "\n",
    "1) Directly from the web-pages themselves, by accessing the pages and then parsing them for information\n",
    "2) Using a provided **API** or Application Programming Interface, which allows for direct access to information from a webpage\n",
    "3) Manually, by copy and pasting material from web-pages\n",
    "4) Via screen capture, which is then usually parsed by a program for requested information\n",
    "\n",
    "In this workshop, we will primarily be discussing the first type of web-scraping, which is the most general.\n",
    "\n",
    "## Legal and Ethical Issues\n",
    "\n",
    "It is **very important** to mention that web scraping can run afoul of both legal and ethical programs.  Legally, the current case law is unsettled, but web-scraping can be illegal in situations including, but not limited to cases where:\n",
    "\n",
    "1) It accesses confidential, private, or protected information - even if that information is public online.\n",
    "2) The access to a website or process is prohibited by the website's owner or the terms and conditions\n",
    "3) The scraping poses an \"undue burden\" on the website, or hinders the use by other individuals\n",
    "4) If it harms the owner of a website or the users of that website in a specific manner\n",
    "\n",
    "It's also important to mention that even in situations where web-scraping is not _illegal_ it may still be unethical; for example, if someone asks you not to, but you do anyways.  \n",
    "\n",
    "### Best Practices\n",
    "\n",
    "In general, we recommend that you should always follow these best practices when scraping websites:\n",
    "\n",
    "1)  Check the terms of conditions to ensure that they permit scraping or spiders\n",
    "2)  Run a scraper in a \"friendly\" manner, limiting the speed and scope as much as feasible\n",
    "2)  **Always** obey a `robots.txt` file, which is a file websites can use to block scrapers\n",
    "4)  **Never** scrape or collect information which is private or not intended to be made public\n",
    "5)  If an API is available for a website, use the API - don't scrape.\n",
    "6)  If you have any doubt, don't do it - consult a lawyer!\n",
    "\n",
    "I am not a lawyer, this is not legal advice, and I make no warranties for any liability or damages you may incur by using web-scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d8b4b0-82f3-4e25-bbfd-506455e01e85",
   "metadata": {},
   "source": [
    "# Markdown\n",
    "\n",
    "Modern websites are written in a variety of languages, but fundamentally rely on _markup_  to display and render content.  Some examples are **HTML** and **XML**.\n",
    "\n",
    "* Markup languages organize content in such a way that the data associated with the content is stored distinctly from the content itself.  For example, the properties of a text (e.g. font, color, bolding) are store seperately from the text itself\n",
    "* This is usually done using **tags** which define the type of content, which have **attributes** that describe them\n",
    "\n",
    "The entire structure of a document can be organized using these tags, providing an easy way from browsers to understand how to display the content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab61880b-ccc2-42d9-ad38-86ab1e147a33",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scraping: DOMs\n",
    "\n",
    "Modern webscraping tools, like the `scrapy` package we will discuss today using the **DOM** or _document object model_ of a website to target their data collection.\n",
    "\n",
    "* Essentially, when you access a website, it will return an HTML document which contains the content with markup\n",
    "* In order to render that content and allow for scripts (like Javascript) your browser creates an internal model which is representation of the DOM\n",
    "* The DOM organizes the content and controls how it is displayed, and how events like user interactions (such as clicking) change that display\n",
    "* The structure of a DOM has been standardized, so that people can write code and websites that will work across many applications\n",
    "\n",
    "You don't need to know a ton about this - but basically modern web scrapers work by identifying what parts of the DOM contain their desired content, then collecting them.\n",
    "\n",
    "##  CSS and XPath\n",
    "\n",
    "Web scrapers identify these elements in two ways: **CSS** (_cascading style sheets_) and **XPath** (_XML Path Language_).\n",
    "\n",
    "* In CSS, elements of the DOM (like a paragraph) have _attributes_ which can be accessed directly\n",
    "  * For example, you may want to collect all paragraphs elements with the attribute `class = body-text`\n",
    "* In XPath, you specify the elements of the DOM desired along an _axis_, then filter them using _predicates_ to choose the key elements\n",
    "  * For example, you may want to collect all `paragraphs` which are children of the `body` with `class = body-text`\n",
    "  \n",
    "In general, XPath is preferred due to the flexibility of the tool, but there are cases where CSS is simpler or better.  Happily, you can mix-and-match the two methods by layering them - so, whatever works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73106be7-e500-4022-982f-ef1ee1bb0e56",
   "metadata": {},
   "source": [
    "# Scraping Workflow\n",
    "\n",
    "In general, one of the hardest parts of web-scraping is not writing the scraper at all: it's identifying what to scrape and how to scrape it.\n",
    "\n",
    "1)  Start by identifying the web pages you want to scrape - a good scraper is _well targeted_\n",
    "2)  Look at the structure of the web pages you want to scrape - how are they related to one another?\n",
    "    * You will particularly need to identify how the scraper will proceed from one page to the next; is there a clear path?\n",
    "3)  Identify what content you want to collect on the website in question and how it exists in the DOM\n",
    "4)  Determine the most efficient way to collect and parse that content with your scraper\n",
    "    * Can you divide the program into \"sub-problems\" which are easier to solve, or find a general solution?\n",
    "5)  Write the code to implement your scraping approach\n",
    "6)  Test and validate the results\n",
    "\n",
    "Notice that nearly half of the steps involve _no coding_ but a deep consideration of the website in question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0348e7f3-1f6d-4831-9049-4a07663b7f72",
   "metadata": {},
   "source": [
    "# Step 2: Inspecting a Website\n",
    "\n",
    "In order to identify the elements necessary, we need to inspect our website.  We're going to use the example from:\n",
    "\n",
    "<https://quotes.toscrape.com/>\n",
    "\n",
    "* _toscrape.com_ is an excellent resource to learn and test scrapers, since it offers a wide range of formats and common \"challenges\" you can encounter\n",
    "\n",
    "When we click on the website, we will see the page.  But how do we find the elements of the DOM needed?  \n",
    "\n",
    "* Right-click the page, then select \"Inspect\" which will open the inspector window.  \n",
    "* This looks very complex, but it's actually very not that bad - it's showing you the DOM\n",
    "* If you click on the \"inspect\" icon, you can observe how the visual layout relates to the DOM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f1635e-e627-424a-a505-fde5a356e0f1",
   "metadata": {},
   "source": [
    "## CSS and XPath Selectors\n",
    "\n",
    "You can use this view to inspect and test different selections.\n",
    "\n",
    "* If you click on \"highlight\" it will show you how specific selectors will behave on the page\n",
    "* If you right-click an element, you can copy XPath and CSS selectors\n",
    "\n",
    "This allows you to build a \"plan of attack\" for the different elements we will need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17198766-d50e-4283-8de6-4391ec6798a8",
   "metadata": {},
   "source": [
    "# Workshop Example: Pulling Quotes\n",
    "\n",
    "Let's suppose we want to extract all of the quote text from this page.  We need to identify an appropriate selector that will collect those elements.\n",
    "\n",
    "* As we can see, the CSS selector `.quote span.text` will collect these elements from the page\n",
    "\n",
    "Note that you often want to be a little bit careful to select appropriately in order to preserve relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3cffbd-a2e6-468a-a443-b9ba3f9ec3a8",
   "metadata": {},
   "source": [
    "# Getting Started with Scrapy\n",
    "\n",
    "Our next task is to get started with `scrapy` which an an excellent, easy-to-use scraping tool: <https://docs.scrapy.org>\n",
    "\n",
    "* Scrapy works using _projects_ which are collections of scraping code.  A Scrapy project consists of:\n",
    "    1) A collection of _spiders_ which crawl the web and collect elements\n",
    "    2) A set of _items_ which define the objects captured by the spider\n",
    "    3) A collection of _pipelines_ which handle and parse the items\n",
    "    4) _middleware_, _configs_, and _settings_ which describe how the spider and pipelines behave\n",
    "\n",
    "* For most projects, the defaults or pre-defined options for _items_, _middleware_, and _pipelines_ are sufficient\n",
    "\n",
    "The main element you need to create are the _spider_ files, and tweaking the settings.\n",
    "\n",
    "## Installing Scrapy\n",
    "\n",
    "You can install Scrapy by opening a terminal and running:\n",
    "\n",
    "```pip install scrapy```\n",
    "\n",
    "If you're on `conda` you can use:\n",
    "\n",
    "```conda install -c conda-forge scrapy```\n",
    "\n",
    "This will install Scrapy on your system.\n",
    "\n",
    "# Setting Up A Scrapy Project\n",
    "\n",
    "You want to make sure Scrapy sets up a project properly.  Fortunately, it has a tool for this: `startproject`\n",
    "\n",
    "1)  Create a new directory where you want your project to live\n",
    "2)  In the terminal, run `scrapy startproject myproject [project_dir]`\n",
    "    * `myproject` is the name of your project\n",
    "    * `project_dir` is optional, but is the name of the folder you want it to be stored in\n",
    "    \n",
    "That's it!  You will see a bunch of default files are created.  Ignore them for now.\n",
    "\n",
    "## Generating a New Spider\n",
    "\n",
    "You can create spider from scratch, it's easier to use a template, since they inherit a lot of code from Scrapy's spider class.  To do this, from the project directory open a terminal and run:\n",
    "\n",
    "```scrapy genspider <name> <domain or URL>```\n",
    "\n",
    "For example, in our case we would use:\n",
    "\n",
    "```scrapy genspider quotespider https://quotes.toscrape.com/page/1/```\n",
    "\n",
    "This will create a blank spider with many options pre-set for us.  We can now edit the spider and set it up!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b926ca16-7fdf-4f37-9459-ba4dd937a0b7",
   "metadata": {},
   "source": [
    "# Testing Selectors Using the Shell\n",
    "\n",
    "Before we set up a spider, we need to check that it will collect items properly.  We can do this using the **scrapy shell** - which is basically a spider's `parse` method running in interactive mode.  To do this, start a terminal then run:\n",
    "\n",
    "```scrapy shell <website>```\n",
    "\n",
    "This will start an interactive session which is basically the DOM accessor of the spider.   We can then test selectors.  Try the following:\n",
    "\n",
    "* `response.css(\".quote\")`\n",
    "* `response.css(\".quote\").getall()`\n",
    "* `response.css(\"div.quote span.text\").getall()`\n",
    "* `response.css(\"div.quote span.text::text\").getall()`\n",
    "\n",
    "Notice that many of these are _lists_ which we can loop over."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16d79a-6330-4aa1-b09c-f40e13d51cf6",
   "metadata": {},
   "source": [
    "## XPath and CSS\n",
    "\n",
    "You can also chain selectors:\n",
    "    \n",
    "```response.css(\"div.quote\").xpath('//span/text()').getall()```\n",
    "\n",
    "You can choose whatever makes things easy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05eee0c-c270-425b-84b8-03b652d5417e",
   "metadata": {},
   "source": [
    "# Adding a Selector to a Spider\n",
    "\n",
    "We can then basically perform a loop to collect the required elements under the `parse` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8107f410-4829-438e-9bd3-12f6e5451ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for quote in response.css(\"div.quote\"):\n",
    "    yield {\n",
    "        'text': quote.css(\"span.text::text\").get()   \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2258470-cb9b-491a-8fc6-35e90befeaf7",
   "metadata": {},
   "source": [
    "# Running Your Spider\n",
    "\n",
    "At this point, we can now run our spider.  Everything that is `yield`-ed from the parse method is sent to the console.  We would prefer that it was stored.\n",
    "\n",
    "* The best format for generic scaped data is JSON, which is natively supported in Scrapy output\n",
    "\n",
    "```scrapy crawl <spide> -O quotes.json```\n",
    "\n",
    "This will generate (and overwrite) a serialize JSON object which has all our information.  Let's try it from the console."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d702a6bc-7a33-4346-a138-f3a2449b72d4",
   "metadata": {},
   "source": [
    "# Be More Spider-y: Following Links\n",
    "\n",
    "You can have you spider crawl the web by following links.  In order to do this, it must be capable of:\n",
    "\n",
    "1) Identifying which links to follow\n",
    "2) Understanding how to the parse the links, once following\n",
    "\n",
    "In our example, all of the Quotes pages are identical, meaning we can use the same parser for each of them - but you might need to write a custom one for different types of pages.\n",
    "\n",
    "Let's start by finding the element that has the next page URL using the inspector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11490e3a-d86a-4a34-a538-8dbf88d6def0",
   "metadata": {},
   "source": [
    "## Next Page\n",
    "\n",
    "In our case, it's `li.next a`.  We can get the HREF from this using the attribute selector:\n",
    "\n",
    "```response.css(\"li.next a\").attrib['href']```\n",
    "\n",
    "Try it interactively in the console to check.  We can then append it using `urljoin` to create a new page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78c4bb-d75f-43b8-adf6-f83333ef7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page = response.css(\"li.next a\").attrib['href'].get()\n",
    "\n",
    "if next_page is not None:\n",
    "    yield response.follow(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc6366-3ccd-4df9-8eac-0552c9e8ed01",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "There are many other examples and patterns on the Scrapy website, which are pretty easy to learn and play around with.\n",
    "\n",
    "* Try to add code which extracts the author and the tags for a quote!\n",
    "* Try to modify the _follow_ code to page odd numbered pages different from even number ones\n",
    "\n",
    "You can get much more sophisticated, but this was the basics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162981ac-c975-405c-bac7-d9fdabe40264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
